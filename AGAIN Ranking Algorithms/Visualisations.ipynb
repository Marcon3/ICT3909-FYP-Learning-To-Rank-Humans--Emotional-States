{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = r'C:\\Users\\marco\\OneDrive\\Desktop\\Final Year Project'\n",
    "os.chdir(project_directory)\n",
    "base_dir = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ci(mean, std_dev, n):\n",
    "    margin_error = 1.96 * (std_dev / np.sqrt(n))\n",
    "    return (mean - margin_error, mean + margin_error)\n",
    "\n",
    "file_dir = 'AGAIN Ranking Algorithms/Evaluation'\n",
    "\n",
    "# File paths\n",
    "algorithm_names = {\n",
    "    'RandomForest/random_forest_evaluation_results.csv': 'Random Forest\\nPreference Learner',\n",
    "    'Ordinal Logistic Regression/OLR_evaluation_result.csv': 'Ordinal Logistic\\nRegression',\n",
    "    'Ordinal Neural Network/ONN_evaluation_results.csv': 'Ordinal Neural\\nNetwork',\n",
    "    'RankNET/ranknet_evaluation_results_Heist!.csv': 'RankNet',\n",
    "    'RankNET/ranknet_evaluation_results_Shootout.csv': 'RankNet',\n",
    "    'RankNET/ranknet_evaluation_results_TopDown.csv': 'RankNet',\n",
    "    'LambdaMart/individual_evaluation_results.csv': 'LambdaMART',\n",
    "    'Regression/linear_regression_evaluation_results.csv': 'Linear\\nRegression',\n",
    "    'Regression/random_forest_evaluation_results.csv': 'Random Forest\\nRegression',\n",
    "    'Regression/mlp_evaluation_results.csv': 'Multi Layer\\nPerceptron Regression',\n",
    "}\n",
    "\n",
    "base_results_dir = 'AGAIN Ranking Algorithms/Evaluation/Visualisations'\n",
    "means_dir = os.path.join(base_results_dir, 'Means')\n",
    "std_devs_dir = os.path.join(base_results_dir, 'StandardDeviations')\n",
    "cis_dir = os.path.join(base_results_dir, 'ConfidenceIntervals')\n",
    "plots_dir = os.path.join(base_results_dir, 'Plots')\n",
    "\n",
    "os.makedirs(means_dir, exist_ok=True)\n",
    "os.makedirs(std_devs_dir, exist_ok=True)\n",
    "os.makedirs(cis_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "for file_name, algorithm_name in algorithm_names.items():\n",
    "    full_path = os.path.join(file_dir, file_name)\n",
    "    if os.path.exists(full_path):\n",
    "        data = pd.read_csv(full_path)\n",
    "        game_column = 'Game Name' if 'Game Name' in data.columns else 'Game'\n",
    "\n",
    "        # Process data for each game\n",
    "        for game in data[game_column].unique():\n",
    "            game_data = data[data[game_column] == game]\n",
    "            game_data = game_data.dropna(subset=['PCC', 'KendallTau'])\n",
    "\n",
    "            if not game_data.empty:\n",
    "                pcc_mean = game_data['PCC'].mean()\n",
    "                kendall_tau_mean = game_data['KendallTau'].mean()\n",
    "                pcc_sd = game_data['PCC'].std()\n",
    "                kendall_tau_sd = game_data['KendallTau'].std()\n",
    "                sample_size = len(game_data)\n",
    "\n",
    "                # Calculate confidence intervals\n",
    "                pcc_ci = calculate_ci(pcc_mean, pcc_sd, sample_size)\n",
    "                kendall_tau_ci = calculate_ci(kendall_tau_mean, kendall_tau_sd, sample_size)\n",
    "            \n",
    "                # Replace invalid characters in file names\n",
    "                safe_algorithm_name = algorithm_name.replace('\\n', '_')\n",
    "\n",
    "                # Save mean, standard deviation, and CI in separate CSVs\n",
    "                mean_df = pd.DataFrame({'PCC': [pcc_mean], 'Kendall Tau': [kendall_tau_mean]})\n",
    "                std_dev_df = pd.DataFrame({'PCC': [pcc_sd], 'Kendall Tau': [kendall_tau_sd]})\n",
    "                ci_df = pd.DataFrame({'PCC': [pcc_ci], 'Kendall Tau': [kendall_tau_ci]})\n",
    "\n",
    "                mean_path = os.path.join(means_dir, f'{safe_algorithm_name}_{game}.csv')\n",
    "                std_dev_path = os.path.join(std_devs_dir, f'{safe_algorithm_name}_{game}.csv')\n",
    "                ci_path = os.path.join(cis_dir, f'{safe_algorithm_name}_{game}.csv')\n",
    "\n",
    "                mean_df.to_csv(mean_path, index=False)\n",
    "                std_dev_df.to_csv(std_dev_path, index=False)\n",
    "                ci_df.to_csv(ci_path, index=False)\n",
    "    \n",
    "                # Store in dictionary for plotting\n",
    "                if game not in model_performance:\n",
    "                    model_performance[game] = {}\n",
    "                model_performance[game][algorithm_name] = (pcc_mean, kendall_tau_mean, pcc_ci, kendall_tau_ci)\n",
    "\n",
    "# Plotting function\n",
    "def plot_results(models, pcc_means, kendall_means, pcc_cis, kendall_cis, game, output_path):\n",
    "    fig, ax = plt.subplots(figsize=(19, 9)) \n",
    "\n",
    "    index = np.arange(len(models))\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    pcc_bars = ax.bar(index - bar_width / 2, pcc_means, bar_width, yerr=[(ci[1] - ci[0]) / 2 for ci in pcc_cis], capsize=5, color='purple', label='PCC')\n",
    "    kendall_bars = ax.bar(index + bar_width / 2, kendall_means, bar_width, yerr=[(ci[1] - ci[0]) / 2 for ci in kendall_cis], capsize=5, color='plum', label='Kendall Tau')\n",
    "    \n",
    "    ax.set_xlabel('Algorithms', fontsize=16)\n",
    "    ax.set_ylabel('Average', fontsize=16)\n",
    "    ax.set_title(f'Averages for {game} Arousal Ranking and Regression Algorithms', fontsize=18)\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(models, rotation=360, ha='center', fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    \n",
    "    plt.ylim(0, max(max(pcc_means), max(kendall_means)) + 0.1) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, f'{game}_comparison.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Plot results for each game\n",
    "for game, algorithms in model_performance.items():\n",
    "    models = list(algorithms.keys())\n",
    "    pcc_means = [alg[0] for alg in algorithms.values()]\n",
    "    kendall_means = [alg[1] for alg in algorithms.values()]\n",
    "    pcc_cis = [alg[2] for alg in algorithms.values()]\n",
    "    kendall_cis = [alg[3] for alg in algorithms.values()]\n",
    "\n",
    "    plot_results(models, pcc_means, kendall_means, pcc_cis, kendall_cis, game, plots_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
