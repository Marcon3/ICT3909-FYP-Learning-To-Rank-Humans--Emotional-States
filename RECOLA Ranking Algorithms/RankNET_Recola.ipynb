{"cells":[{"cell_type":"markdown","metadata":{"id":"SfTj8BrwMavO"},"source":["# RankNET (RECOLA)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11180,"status":"ok","timestamp":1711053726762,"user":{"displayName":"Marcon Spiteri","userId":"03188949906581882183"},"user_tz":-60},"id":"md4ZUHSoMgsU"},"outputs":[],"source":["import os\n","import gc \n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.nn import leaky_relu\n","from sklearn.metrics import accuracy_score\n","from scipy.stats import pearsonr, kendalltau\n","from sklearn.model_selection import GroupKFold\n","from tensorflow.keras import layers, Model, Input"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["project_directory = r'C:\\Users\\marco\\OneDrive\\Desktop\\Final Year Project'\n","os.chdir(project_directory)\n","base_dir = os.getcwd() "]},{"cell_type":"markdown","metadata":{},"source":["- Evaluation for arousal"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Processing for arousal_quartile:\n","\n","Fold 1/5 for arousal_quartile:\n","Epoch 1/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1ms/step - loss: 234.1577\n","Epoch 2/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1ms/step - loss: 3.6597\n","\u001b[1m2602/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 834us/step\n","\u001b[1m1561/1561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 988us/step\n","\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","\n","Fold 2/5 for arousal_quartile:\n","Epoch 1/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 1ms/step - loss: 1.7831\n","Epoch 2/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1ms/step - loss: 0.6009\n","\u001b[1m2602/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 824us/step\n","\u001b[1m1561/1561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step\n","\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step\n","\n","Fold 3/5 for arousal_quartile:\n","Epoch 1/2\n","\u001b[1m101300/101300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 1ms/step - loss: 0.4581\n","Epoch 2/2\n","\u001b[1m101300/101300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1ms/step - loss: 0.4320\n","\u001b[1m3629/3629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 860us/step\n","\u001b[1m2588/2588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 829us/step\n","\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step\n","\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step\n","\n","Fold 4/5 for arousal_quartile:\n","Epoch 1/2\n","\u001b[1m101398/101398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 1ms/step - loss: 0.4254\n","Epoch 2/2\n","\u001b[1m101398/101398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1ms/step - loss: 0.4006\n","\u001b[1m3622/3622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 815us/step\n","\u001b[1m2567/2567\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 872us/step\n","\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step\n","\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step\n","\n","Fold 5/5 for arousal_quartile:\n","Epoch 1/2\n","\u001b[1m101495/101495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 1ms/step - loss: 0.3781\n","Epoch 2/2\n","\u001b[1m101495/101495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1ms/step - loss: 0.3603\n","\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811us/step\n","\u001b[1m2581/2581\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 841us/step\n","\u001b[1m1540/1540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806us/step\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step\n","Individual performance evaluation arousal results saved.\n"]}],"source":["def pairwise_transformation(X, Y, groups):\n","    transformed_data = []\n","    transformed_labels = []\n","    transformed_groups = []\n","    for i in range(len(X)):\n","        for j in range(i+1, len(X)):\n","            xi, xj = X[i], X[j]\n","            yi, yj = Y[i], Y[j]\n","            if np.abs(yi - yj) > 0.1:\n","                pair1 = np.concatenate([xi, xj])\n","                pair2 = np.concatenate([xj, xi])\n","                transformed_data.append(pair1)\n","                transformed_data.append(pair2)\n","\n","                transformed_groups.append(groups[i])\n","                transformed_groups.append(groups[i])\n","\n","                if yi > yj:\n","                    transformed_labels.append(1)\n","                    transformed_labels.append(0)\n","                else:\n","                    transformed_labels.append(0)\n","                    transformed_labels.append(1)\n","\n","    return np.array(transformed_data), np.array(transformed_labels), np.array(transformed_groups)\n","\n","\n","class RankNet(Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.dense = [layers.Dense(16, activation=leaky_relu), layers.Dense(8, activation=leaky_relu)]\n","        self.o = layers.Dense(1, activation='linear')\n","        self.oi_minus_oj = layers.Subtract()\n","\n","    def call(self, input_tensor):\n","        half = input_tensor.shape[-1] // 2\n","        xi, xj = input_tensor[:, :half], input_tensor[:, half:]\n","        densei = self.dense[0](xi)\n","        densej = self.dense[0](xj)\n","        for dense in self.dense[1:]:\n","            densei = dense(densei)\n","            densej = dense(densej)\n","        oi = self.o(densei)\n","        oj = self.o(densej)\n","        oij = self.oi_minus_oj([oi, oj])\n","        output = layers.Activation('sigmoid')(oij)\n","        return output\n","\n","input_file = os.path.join('RECOLA Ranking Algorithms', 'RECOLA_Intervals_Data', 'ArousalValenceTimeSeries_Quartiles.csv')\n","df = pd.read_csv(input_file)\n","\n","def concordance_correlation_coefficient(y_true, y_pred):\n","    cor = np.corrcoef(y_true, y_pred)[0][1]\n","    mean_true, mean_pred = np.mean(y_true), np.mean(y_pred)\n","    var_true, var_pred = np.var(y_true), np.var(y_pred)\n","    sd_true, sd_pred = np.std(y_true), np.std(y_pred)\n","    numerator = 2 * cor * sd_true * sd_pred\n","    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n","    ccc = numerator / denominator\n","    return ccc\n","\n","def pearson_correlation_coefficient(y_true, y_pred):\n","    pcc, _ = pearsonr(y_true, y_pred)\n","    return pcc\n","\n","def kendalls_tau_coefficient(y_true, y_pred):\n","    tau, _ = kendalltau(y_true, y_pred)\n","    return tau\n","\n","# Evaluation function \n","def evaluate_ranknet_performance(ranknet_model, X_transformed, Y, groups):\n","    evaluation_results = []\n","    unique_groups = np.unique(groups)\n","    for group_id in unique_groups:\n","        idx = groups == group_id\n","        group_features = X_transformed[idx]\n","        group_labels = Y[idx]\n","\n","        mean_features = group_features.mean(axis=0)\n","        normalized_features = group_features - mean_features\n","   \n","        predicted_scores = ranknet_model.predict(normalized_features).flatten()\n","   \n","        ccc_value = concordance_correlation_coefficient(group_labels, predicted_scores)\n","        pcc_value = pearson_correlation_coefficient(group_labels, predicted_scores)\n","        kendall_tau_value = kendalls_tau_coefficient(group_labels, predicted_scores)\n","   \n","        evaluation_results.append({'GroupID': group_id, 'CCC': ccc_value, 'PCC': pcc_value, 'KendallTau': kendall_tau_value})\n","    return pd.DataFrame(evaluation_results)\n","\n","evaluation_results = []  \n","\n","# Initialize the RankNet model\n","ranknet_model = RankNet()\n","ranknet_model.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","excluded_features = [\n","        'VIDEO_40_LLD_AU1', 'VIDEO_40_LLD_AU2', 'VIDEO_40_LLD_AU4', 'VIDEO_40_LLD_AU5', 'VIDEO_40_LLD_AU6', 'VIDEO_40_LLD_AU7', 'VIDEO_40_LLD_AU9', 'VIDEO_40_LLD_AU11', 'VIDEO_40_LLD_AU12', 'VIDEO_40_LLD_AU15',\n","        'VIDEO_40_LLD_AU17', 'VIDEO_40_LLD_AU20', 'VIDEO_40_LLD_AU23', 'VIDEO_40_LLD_AU24', 'VIDEO_40_LLD_AU25', 'VIDEO_40_LLD_Yaw', 'VIDEO_40_LLD_Pitch', 'VIDEO_40_LLD_Roll', 'VIDEO_40_LLD_Opt_mean',\n","        'VIDEO_40_LLD_Opt_std', 'VIDEO_40_LLD_AU1_delta', 'VIDEO_40_LLD_AU2_delta', 'VIDEO_40_LLD_AU4_delta', 'VIDEO_40_LLD_AU5_delta', 'VIDEO_40_LLD_AU6_delta', 'VIDEO_40_LLD_AU7_delta', 'VIDEO_40_LLD_AU9_delta',\n","        'VIDEO_40_LLD_AU11_delta', 'VIDEO_40_LLD_AU12_delta', 'VIDEO_40_LLD_AU15_delta', 'VIDEO_40_LLD_AU17_delta', 'VIDEO_40_LLD_AU20_delta', 'VIDEO_40_LLD_AU23_delta', 'VIDEO_40_LLD_AU24_delta', 'VIDEO_40_LLD_AU25_delta',\n","        'VIDEO_40_LLD_Yaw_delta', 'VIDEO_40_LLD_Pitch_delta', 'VIDEO_40_LLD_Roll_delta', 'VIDEO_40_LLD_Opt_mean_delta', 'VIDEO_40_LLD_Opt_std_delta', 'Face_detection_probability', 'ECG_54_LLD_ECG_HR', 'ECG_54_LLD_ECG_HRV',\n","        'ECG_54_LLD_ECG_zcr', 'ECG_54_LLD_ECG_FFT_1', 'ECG_54_LLD_ECG_FFT_2', 'ECG_54_LLD_ECG_FFT_3', 'ECG_54_LLD_ECG_FFT_4', 'ECG_54_LLD_ECG_FFT_5', 'ECG_54_LLD_ECG_FFT_6', 'ECG_54_LLD_ECG_FFT_7', 'ECG_54_LLD_ECG_FFT_8',\n","        'ECG_54_LLD_ECG_FFT_9', 'ECG_54_LLD_ECG_FFT_10', 'ECG_54_LLD_ECG_FFT_11', 'ECG_54_LLD_ECG_FFT_12', 'ECG_54_LLD_ECG_FFT_entropy', 'ECG_54_LLD_ECG_FFT_mean_frequency', 'ECG_54_LLD_ECG_FFT_slope', 'ECG_54_LLD_ECG_mean',\n","        'ECG_54_LLD_ECG_std', 'ECG_54_LLD_ECG_kurtosis', 'ECG_54_LLD_ECG_skewness', 'ECG_54_LLD_ECG_NSImn', 'ECG_54_LLD_ECG_NLDmn', 'ECG_54_LLD_ECG_VLF', 'ECG_54_LLD_ECG_LF', 'ECG_54_LLD_ECG_HF', 'ECG_54_LLD_ECG_LFHF', 'ECG_54_LLD_ECG_zcr_delta',\n","        'ECG_54_LLD_ECG_FFT_1_delta', 'ECG_54_LLD_ECG_FFT_2_delta', 'ECG_54_LLD_ECG_FFT_3_delta', 'ECG_54_LLD_ECG_FFT_4_delta', 'ECG_54_LLD_ECG_FFT_5_delta', 'ECG_54_LLD_ECG_FFT_6_delta', 'ECG_54_LLD_ECG_FFT_7_delta', 'ECG_54_LLD_ECG_FFT_8_delta',\n","        'ECG_54_LLD_ECG_FFT_9_delta', 'ECG_54_LLD_ECG_FFT_10_delta', 'ECG_54_LLD_ECG_FFT_11_delta', 'ECG_54_LLD_ECG_FFT_12_delta', 'ECG_54_LLD_ECG_FFT_entropy_delta', 'ECG_54_LLD_ECG_FFT_mean_frequency_delta', 'ECG_54_LLD_ECG_FFT_slope_delta', 'ECG_54_LLD_ECG_mean_delta',\n","        'ECG_54_LLD_ECG_std_delta', 'ECG_54_LLD_ECG_kurtosis_delta', 'ECG_54_LLD_ECG_skewness_delta', 'ECG_54_LLD_ECG_NSImn_delta', 'ECG_54_LLD_ECG_NLDmn_delta', 'ECG_54_LLD_ECG_VLF_delta', 'ECG_54_LLD_ECG_LF_delta', 'ECG_54_LLD_ECG_HF_delta', 'ECG_54_LLD_ECG_LFHF_delta',\n","        'EDA_62_LLD_time_code', 'EDA_62_LLD_EDA_slope', 'EDA_62_LLD_EDA_std', 'EDA_62_LLD_SCR_FFT_entropy', 'EDA_62_LLD_SCR_FFT_mean_frequency', 'EDA_62_LLD_EDA_mean', 'EDA_62_LLD_EDA_meanD', 'EDA_62_LLD_EDA_meanDneg', 'EDA_62_LLD_EDA_prop', 'EDA_62_LLD_EDA_Xbound', 'EDA_62_LLD_EDA_kurtosis',\n","        'EDA_62_LLD_EDA_skewness', 'EDA_62_LLD_EDA_NSImn', 'EDA_62_LLD_EDA_NLDmn', 'EDA_62_LLD_SCL_mean', 'EDA_62_LLD_SCL_meanD', 'EDA_62_LLD_SCL_meanDneg', 'EDA_62_LLD_SCL_prop', 'EDA_62_LLD_SCL_Xbound', 'EDA_62_LLD_SCL_kurtosis', 'EDA_62_LLD_SCL_skewness', 'EDA_62_LLD_SCL_NSImn',\n","        'EDA_62_LLD_SCL_NLDmn', 'EDA_62_LLD_SCR_mean', 'EDA_62_LLD_SCR_meanD', 'EDA_62_LLD_SCR_meanDneg', 'EDA_62_LLD_SCR_prop', 'EDA_62_LLD_SCR_Xbound', 'EDA_62_LLD_SCR_kurtosis', 'EDA_62_LLD_SCR_skewness', 'EDA_62_LLD_SCR_NSImn', 'EDA_62_LLD_SCR_NLDmn', 'EDA_62_LLD_EDA_slope_delta',\n","        'EDA_62_LLD_EDA_std_delta', 'EDA_62_LLD_SCR_FFT_entropy_delta', 'EDA_62_LLD_SCR_FFT_mean_frequency_delta', 'EDA_62_LLD_EDA_mean_delta', 'EDA_62_LLD_EDA_meanD_delta', 'EDA_62_LLD_EDA_meanDneg_delta', 'EDA_62_LLD_EDA_prop_delta', 'EDA_62_LLD_EDA_Xbound_delta', 'EDA_62_LLD_EDA_kurtosis_delta',\n","        'EDA_62_LLD_EDA_skewness_delta', 'EDA_62_LLD_EDA_NSImn_delta', 'EDA_62_LLD_EDA_NLDmn_delta', 'EDA_62_LLD_SCL_mean_delta', 'EDA_62_LLD_SCL_meanD_delta', 'EDA_62_LLD_SCL_meanDneg_delta', 'EDA_62_LLD_SCL_prop_delta', 'EDA_62_LLD_SCL_Xbound_delta', 'EDA_62_LLD_SCL_kurtosis_delta', 'EDA_62_LLD_SCL_skewness_delta',\n","        'EDA_62_LLD_SCL_NSImn_delta', 'EDA_62_LLD_SCL_NLDmn_delta', 'EDA_62_LLD_SCR_mean_delta', 'EDA_62_LLD_SCR_meanD_delta', 'EDA_62_LLD_SCR_meanDneg_delta', 'EDA_62_LLD_SCR_prop_delta', 'EDA_62_LLD_SCR_Xbound_delta', 'EDA_62_LLD_SCR_kurtosis_delta', 'EDA_62_LLD_SCR_skewness_delta', 'EDA_62_LLD_SCR_NSImn_delta', 'EDA_62_LLD_SCR_NLDmn_delta'\n","\n","    ]\n","\n","feature_columns = [col for col in df.columns if col not in excluded_features + ['participant_id','median_arousal', 'median_valence', 'time_window', 'arousal_quartile', 'valence_quartile']]\n","\n","targets = ['arousal_quartile']\n","\n","# Initialize GroupKFold\n","group_kfold = GroupKFold(n_splits=5)\n","\n","# Process for arousal_quartile\n","for target in targets:\n","    print(f\"\\nProcessing for {target}:\")\n","    X = df[feature_columns].values\n","    Y = df[target].values\n","    groups = df['participant_id'].values\n","    for fold, (train_index, test_index) in enumerate(group_kfold.split(X, Y, groups=groups)):\n","        print(f\"\\nFold {fold+1}/{group_kfold.n_splits} for {target}:\")\n","        X_train, X_test = X[train_index], X[test_index]\n","        Y_train, Y_test = Y[train_index], Y[test_index]\n","        groups_train, groups_test = groups[train_index], groups[test_index]\n","\n","        # Transform both training and testing data along with groups\n","        X_train_transformed, Y_train_transformed, groups_train_transformed = pairwise_transformation(X_train, Y_train, groups_train)\n","        X_test_transformed, Y_test_transformed, groups_test_transformed = pairwise_transformation(X_test, Y_test, groups_test)\n","\n","        ranknet_model.fit(X_train_transformed, Y_train_transformed, epochs=2, verbose=1)\n","\n","        evaluation_df = evaluate_ranknet_performance(ranknet_model, X_test_transformed, Y_test_transformed, groups_test_transformed)\n","        evaluation_results.append(evaluation_df)\n","        del X_train_transformed, Y_train_transformed\n","        del X_test_transformed, Y_test_transformed     \n","        gc.collect()\n","\n","# Combine and save evaluation results\n","combined_results_df = pd.concat(evaluation_results, ignore_index=True)\n","\n","combined_evaluation_results_file = os.path.join('RECOLA Ranking Algorithms','Evaluation','RankNET', 'ranknet_evaluation_results_arousal.csv')\n","combined_results_df.to_csv(combined_evaluation_results_file, index=False)\n","\n","print(\"Individual performance evaluation arousal results saved.\")"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluation for valence"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Processing for valence_quartile:\n","\n","Fold 1/5 for valence_quartile:\n","Epoch 1/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1ms/step - loss: 493.8092\n","Epoch 2/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1ms/step - loss: 0.6754\n","\u001b[1m2602/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 970us/step\n","\u001b[1m1561/1561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n","\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","\n","Fold 2/5 for valence_quartile:\n","Epoch 1/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1ms/step - loss: 0.6148\n","Epoch 2/2\n","\u001b[1m116133/116133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1ms/step - loss: 0.5528\n","\u001b[1m2602/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 877us/step\n","\u001b[1m1561/1561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 909us/step\n","\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step\n","\n","Fold 3/5 for valence_quartile:\n","Epoch 1/2\n","\u001b[1m101300/101300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 1ms/step - loss: 0.4881\n","Epoch 2/2\n","\u001b[1m101300/101300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1ms/step - loss: 0.4579\n","\u001b[1m3629/3629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 833us/step\n","\u001b[1m2588/2588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step\n","\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step\n","\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step\n","\n","Fold 4/5 for valence_quartile:\n","Epoch 1/2\n","\u001b[1m101398/101398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1ms/step - loss: 0.4747\n","Epoch 2/2\n","\u001b[1m101398/101398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1ms/step - loss: 0.4514\n","\u001b[1m3622/3622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 814us/step\n","\u001b[1m2567/2567\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 773us/step\n","\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768us/step\n","\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step\n","\n","Fold 5/5 for valence_quartile:\n","Epoch 1/2\n","\u001b[1m101495/101495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1ms/step - loss: 0.4440\n","Epoch 2/2\n","\u001b[1m101495/101495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1ms/step - loss: 0.4319\n","\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 826us/step\n","\u001b[1m2581/2581\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 815us/step\n","\u001b[1m1540/1540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step\n","\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step\n","Individual performance evaluation valence results saved.\n"]}],"source":["base_dir = 'RECOLA Ranking Algorithms'\n","\n","def pairwise_transformation(X, Y, groups):\n","    transformed_data = []\n","    transformed_labels = []\n","    transformed_groups = []\n","    for i in range(len(X)):\n","        for j in range(i+1, len(X)):\n","            xi, xj = X[i], X[j]\n","            yi, yj = Y[i], Y[j]\n","            if np.abs(yi - yj) > 0.1:\n","                pair1 = np.concatenate([xi, xj])\n","                pair2 = np.concatenate([xj, xi])\n","                transformed_data.append(pair1)\n","                transformed_data.append(pair2)\n","\n","                transformed_groups.append(groups[i])\n","                transformed_groups.append(groups[i])\n","\n","                if yi > yj:\n","                    transformed_labels.append(1)\n","                    transformed_labels.append(0)\n","                else:\n","                    transformed_labels.append(0)\n","                    transformed_labels.append(1)\n","\n","    return np.array(transformed_data), np.array(transformed_labels), np.array(transformed_groups)\n","\n","class RankNet(Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.dense = [layers.Dense(16, activation=leaky_relu), layers.Dense(8, activation=leaky_relu)]\n","        self.o = layers.Dense(1, activation='linear')\n","        self.oi_minus_oj = layers.Subtract()\n","\n","    def call(self, input_tensor):\n","        half = input_tensor.shape[-1] // 2\n","        xi, xj = input_tensor[:, :half], input_tensor[:, half:]\n","        densei = self.dense[0](xi)\n","        densej = self.dense[0](xj)\n","        for dense in self.dense[1:]:\n","            densei = dense(densei)\n","            densej = dense(densej)\n","        oi = self.o(densei)\n","        oj = self.o(densej)\n","        oij = self.oi_minus_oj([oi, oj])\n","        output = layers.Activation('sigmoid')(oij)\n","        return output\n","\n","input_file = os.path.join(base_dir, 'RECOLA_Intervals_Data', 'ArousalValenceTimeSeries_Quartiles.csv')\n","df = pd.read_csv(input_file)\n","\n","def concordance_correlation_coefficient(y_true, y_pred):\n","    cor = np.corrcoef(y_true, y_pred)[0][1]\n","    mean_true, mean_pred = np.mean(y_true), np.mean(y_pred)\n","    var_true, var_pred = np.var(y_true), np.var(y_pred)\n","    sd_true, sd_pred = np.std(y_true), np.std(y_pred)\n","    numerator = 2 * cor * sd_true * sd_pred\n","    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n","    ccc = numerator / denominator\n","    return ccc\n","\n","def pearson_correlation_coefficient(y_true, y_pred):\n","    pcc, _ = pearsonr(y_true, y_pred)\n","    return pcc\n","\n","def kendalls_tau_coefficient(y_true, y_pred):\n","    tau, _ = kendalltau(y_true, y_pred)\n","    return tau\n","\n","def evaluate_ranknet_performance(ranknet_model, X_transformed, Y, groups):\n","    evaluation_results = []\n","    unique_groups = np.unique(groups)\n","    for group_id in unique_groups:\n","        idx = groups == group_id\n","        group_features = X_transformed[idx]\n","        group_labels = Y[idx]\n","\n","        mean_features = group_features.mean(axis=0)\n","        normalized_features = group_features - mean_features\n","   \n","        predicted_scores = ranknet_model.predict(normalized_features).flatten()\n","   \n","        ccc_value = concordance_correlation_coefficient(group_labels, predicted_scores)\n","        pcc_value = pearson_correlation_coefficient(group_labels, predicted_scores)\n","        kendall_tau_value = kendalls_tau_coefficient(group_labels, predicted_scores)\n","   \n","        evaluation_results.append({'GroupID': group_id, 'CCC': ccc_value, 'PCC': pcc_value, 'KendallTau': kendall_tau_value})\n","    return pd.DataFrame(evaluation_results)\n","\n","evaluation_results = []  \n","\n","# Initialize the RankNet model\n","ranknet_model = RankNet()\n","ranknet_model.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","excluded_features = [\n","        'VIDEO_40_LLD_AU1', 'VIDEO_40_LLD_AU2', 'VIDEO_40_LLD_AU4', 'VIDEO_40_LLD_AU5', 'VIDEO_40_LLD_AU6', 'VIDEO_40_LLD_AU7', 'VIDEO_40_LLD_AU9', 'VIDEO_40_LLD_AU11', 'VIDEO_40_LLD_AU12', 'VIDEO_40_LLD_AU15',\n","        'VIDEO_40_LLD_AU17', 'VIDEO_40_LLD_AU20', 'VIDEO_40_LLD_AU23', 'VIDEO_40_LLD_AU24', 'VIDEO_40_LLD_AU25', 'VIDEO_40_LLD_Yaw', 'VIDEO_40_LLD_Pitch', 'VIDEO_40_LLD_Roll', 'VIDEO_40_LLD_Opt_mean',\n","        'VIDEO_40_LLD_Opt_std', 'VIDEO_40_LLD_AU1_delta', 'VIDEO_40_LLD_AU2_delta', 'VIDEO_40_LLD_AU4_delta', 'VIDEO_40_LLD_AU5_delta', 'VIDEO_40_LLD_AU6_delta', 'VIDEO_40_LLD_AU7_delta', 'VIDEO_40_LLD_AU9_delta',\n","        'VIDEO_40_LLD_AU11_delta', 'VIDEO_40_LLD_AU12_delta', 'VIDEO_40_LLD_AU15_delta', 'VIDEO_40_LLD_AU17_delta', 'VIDEO_40_LLD_AU20_delta', 'VIDEO_40_LLD_AU23_delta', 'VIDEO_40_LLD_AU24_delta', 'VIDEO_40_LLD_AU25_delta',\n","        'VIDEO_40_LLD_Yaw_delta', 'VIDEO_40_LLD_Pitch_delta', 'VIDEO_40_LLD_Roll_delta', 'VIDEO_40_LLD_Opt_mean_delta', 'VIDEO_40_LLD_Opt_std_delta', 'Face_detection_probability', 'ECG_54_LLD_ECG_HR', 'ECG_54_LLD_ECG_HRV',\n","        'ECG_54_LLD_ECG_zcr', 'ECG_54_LLD_ECG_FFT_1', 'ECG_54_LLD_ECG_FFT_2', 'ECG_54_LLD_ECG_FFT_3', 'ECG_54_LLD_ECG_FFT_4', 'ECG_54_LLD_ECG_FFT_5', 'ECG_54_LLD_ECG_FFT_6', 'ECG_54_LLD_ECG_FFT_7', 'ECG_54_LLD_ECG_FFT_8',\n","        'ECG_54_LLD_ECG_FFT_9', 'ECG_54_LLD_ECG_FFT_10', 'ECG_54_LLD_ECG_FFT_11', 'ECG_54_LLD_ECG_FFT_12', 'ECG_54_LLD_ECG_FFT_entropy', 'ECG_54_LLD_ECG_FFT_mean_frequency', 'ECG_54_LLD_ECG_FFT_slope', 'ECG_54_LLD_ECG_mean',\n","        'ECG_54_LLD_ECG_std', 'ECG_54_LLD_ECG_kurtosis', 'ECG_54_LLD_ECG_skewness', 'ECG_54_LLD_ECG_NSImn', 'ECG_54_LLD_ECG_NLDmn', 'ECG_54_LLD_ECG_VLF', 'ECG_54_LLD_ECG_LF', 'ECG_54_LLD_ECG_HF', 'ECG_54_LLD_ECG_LFHF', 'ECG_54_LLD_ECG_zcr_delta',\n","        'ECG_54_LLD_ECG_FFT_1_delta', 'ECG_54_LLD_ECG_FFT_2_delta', 'ECG_54_LLD_ECG_FFT_3_delta', 'ECG_54_LLD_ECG_FFT_4_delta', 'ECG_54_LLD_ECG_FFT_5_delta', 'ECG_54_LLD_ECG_FFT_6_delta', 'ECG_54_LLD_ECG_FFT_7_delta', 'ECG_54_LLD_ECG_FFT_8_delta',\n","        'ECG_54_LLD_ECG_FFT_9_delta', 'ECG_54_LLD_ECG_FFT_10_delta', 'ECG_54_LLD_ECG_FFT_11_delta', 'ECG_54_LLD_ECG_FFT_12_delta', 'ECG_54_LLD_ECG_FFT_entropy_delta', 'ECG_54_LLD_ECG_FFT_mean_frequency_delta', 'ECG_54_LLD_ECG_FFT_slope_delta', 'ECG_54_LLD_ECG_mean_delta',\n","        'ECG_54_LLD_ECG_std_delta', 'ECG_54_LLD_ECG_kurtosis_delta', 'ECG_54_LLD_ECG_skewness_delta', 'ECG_54_LLD_ECG_NSImn_delta', 'ECG_54_LLD_ECG_NLDmn_delta', 'ECG_54_LLD_ECG_VLF_delta', 'ECG_54_LLD_ECG_LF_delta', 'ECG_54_LLD_ECG_HF_delta', 'ECG_54_LLD_ECG_LFHF_delta',\n","        'EDA_62_LLD_time_code', 'EDA_62_LLD_EDA_slope', 'EDA_62_LLD_EDA_std', 'EDA_62_LLD_SCR_FFT_entropy', 'EDA_62_LLD_SCR_FFT_mean_frequency', 'EDA_62_LLD_EDA_mean', 'EDA_62_LLD_EDA_meanD', 'EDA_62_LLD_EDA_meanDneg', 'EDA_62_LLD_EDA_prop', 'EDA_62_LLD_EDA_Xbound', 'EDA_62_LLD_EDA_kurtosis',\n","        'EDA_62_LLD_EDA_skewness', 'EDA_62_LLD_EDA_NSImn', 'EDA_62_LLD_EDA_NLDmn', 'EDA_62_LLD_SCL_mean', 'EDA_62_LLD_SCL_meanD', 'EDA_62_LLD_SCL_meanDneg', 'EDA_62_LLD_SCL_prop', 'EDA_62_LLD_SCL_Xbound', 'EDA_62_LLD_SCL_kurtosis', 'EDA_62_LLD_SCL_skewness', 'EDA_62_LLD_SCL_NSImn',\n","        'EDA_62_LLD_SCL_NLDmn', 'EDA_62_LLD_SCR_mean', 'EDA_62_LLD_SCR_meanD', 'EDA_62_LLD_SCR_meanDneg', 'EDA_62_LLD_SCR_prop', 'EDA_62_LLD_SCR_Xbound', 'EDA_62_LLD_SCR_kurtosis', 'EDA_62_LLD_SCR_skewness', 'EDA_62_LLD_SCR_NSImn', 'EDA_62_LLD_SCR_NLDmn', 'EDA_62_LLD_EDA_slope_delta',\n","        'EDA_62_LLD_EDA_std_delta', 'EDA_62_LLD_SCR_FFT_entropy_delta', 'EDA_62_LLD_SCR_FFT_mean_frequency_delta', 'EDA_62_LLD_EDA_mean_delta', 'EDA_62_LLD_EDA_meanD_delta', 'EDA_62_LLD_EDA_meanDneg_delta', 'EDA_62_LLD_EDA_prop_delta', 'EDA_62_LLD_EDA_Xbound_delta', 'EDA_62_LLD_EDA_kurtosis_delta',\n","        'EDA_62_LLD_EDA_skewness_delta', 'EDA_62_LLD_EDA_NSImn_delta', 'EDA_62_LLD_EDA_NLDmn_delta', 'EDA_62_LLD_SCL_mean_delta', 'EDA_62_LLD_SCL_meanD_delta', 'EDA_62_LLD_SCL_meanDneg_delta', 'EDA_62_LLD_SCL_prop_delta', 'EDA_62_LLD_SCL_Xbound_delta', 'EDA_62_LLD_SCL_kurtosis_delta', 'EDA_62_LLD_SCL_skewness_delta',\n","        'EDA_62_LLD_SCL_NSImn_delta', 'EDA_62_LLD_SCL_NLDmn_delta', 'EDA_62_LLD_SCR_mean_delta', 'EDA_62_LLD_SCR_meanD_delta', 'EDA_62_LLD_SCR_meanDneg_delta', 'EDA_62_LLD_SCR_prop_delta', 'EDA_62_LLD_SCR_Xbound_delta', 'EDA_62_LLD_SCR_kurtosis_delta', 'EDA_62_LLD_SCR_skewness_delta', 'EDA_62_LLD_SCR_NSImn_delta', 'EDA_62_LLD_SCR_NLDmn_delta'\n","\n","    ]\n","\n","feature_columns = [col for col in df.columns if col not in excluded_features + ['participant_id','median_arousal', 'median_valence', 'time_window', 'arousal_quartile', 'valence_quartile']]\n","\n","targets = ['valence_quartile']\n","\n","# Initialize GroupKFold\n","group_kfold = GroupKFold(n_splits=5)\n","\n","# Process for valence_quartile\n","for target in targets:\n","    print(f\"\\nProcessing for {target}:\")\n","    X = df[feature_columns].values\n","    Y = df[target].values\n","    groups = df['participant_id'].values\n","    for fold, (train_index, test_index) in enumerate(group_kfold.split(X, Y, groups=groups)):\n","        print(f\"\\nFold {fold+1}/{group_kfold.n_splits} for {target}:\")\n","        X_train, X_test = X[train_index], X[test_index]\n","        Y_train, Y_test = Y[train_index], Y[test_index]\n","        groups_train, groups_test = groups[train_index], groups[test_index]\n","\n","        # Transform both training and testing data along with groups\n","        X_train_transformed, Y_train_transformed, groups_train_transformed = pairwise_transformation(X_train, Y_train, groups_train)\n","        X_test_transformed, Y_test_transformed, groups_test_transformed = pairwise_transformation(X_test, Y_test, groups_test)\n","\n","        ranknet_model.fit(X_train_transformed, Y_train_transformed, epochs=2, verbose=1)\n","\n","        # Insert evaluation after predictions\n","        evaluation_df = evaluate_ranknet_performance(ranknet_model, X_test_transformed, Y_test_transformed, groups_test_transformed)\n","        evaluation_results.append(evaluation_df)\n","        del X_train_transformed, Y_train_transformed\n","        del X_test_transformed, Y_test_transformed     \n","        gc.collect()\n","        \n","        \n","# Combine and save evaluation results\n","combined_results_df = pd.concat(evaluation_results, ignore_index=True)\n","\n","combined_evaluation_results_file = os.path.join('RECOLA Ranking Algorithms','Evaluation','RankNET', 'ranknet_evaluation_results_valence.csv')\n","combined_results_df.to_csv(combined_evaluation_results_file, index=False)\n","\n","print(\"Individual performance evaluation valence results saved.\")\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Processing for valence_quartile:\n","\n","Fold 1/5 for valence_quartile:\n"]}],"source":["base_dir = 'RECOLA Ranking Algorithms'\n","\n","def pairwise_transformation(X, Y, groups):\n","    transformed_data = []\n","    transformed_labels = []\n","    transformed_groups = []\n","    for i in range(len(X)):\n","        for j in range(i+1, len(X)):\n","            xi, xj = X[i], X[j]\n","            yi, yj = Y[i], Y[j]\n","            if np.abs(yi - yj) > 0.1:\n","                pair1 = np.concatenate([xi, xj])\n","                pair2 = np.concatenate([xj, xi])\n","                transformed_data.append(pair1)\n","                transformed_data.append(pair2)\n","\n","                transformed_groups.append(groups[i])\n","                transformed_groups.append(groups[i])\n","\n","                if yi > yj:\n","                    transformed_labels.append(1)\n","                    transformed_labels.append(0)\n","                else:\n","                    transformed_labels.append(0)\n","                    transformed_labels.append(1)\n","\n","    return np.array(transformed_data), np.array(transformed_labels), np.array(transformed_groups)\n","\n","class RankNet(Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.dense = [layers.Dense(16, activation=leaky_relu), layers.Dense(8, activation=leaky_relu)]\n","        self.o = layers.Dense(1, activation='linear')\n","        self.oi_minus_oj = layers.Subtract()\n","\n","    def call(self, input_tensor):\n","        half = input_tensor.shape[-1] // 2\n","        xi, xj = input_tensor[:, :half], input_tensor[:, half:]\n","        densei = self.dense[0](xi)\n","        densej = self.dense[0](xj)\n","        for dense in self.dense[1:]:\n","            densei = dense(densei)\n","            densej = dense(densej)\n","        oi = self.o(densei)\n","        oj = self.o(densej)\n","        oij = self.oi_minus_oj([oi, oj])\n","        output = layers.Activation('sigmoid')(oij)\n","        return output\n","\n","input_file = os.path.join(base_dir, 'RECOLA_Intervals_Data', 'ArousalValenceTimeSeries_Quartiles.csv')\n","df = pd.read_csv(input_file)\n","\n","def concordance_correlation_coefficient(y_true, y_pred):\n","    cor = np.corrcoef(y_true, y_pred)[0][1]\n","    mean_true, mean_pred = np.mean(y_true), np.mean(y_pred)\n","    var_true, var_pred = np.var(y_true), np.var(y_pred)\n","    sd_true, sd_pred = np.std(y_true), np.std(y_pred)\n","    numerator = 2 * cor * sd_true * sd_pred\n","    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n","    ccc = numerator / denominator\n","    return ccc\n","\n","def pearson_correlation_coefficient(y_true, y_pred):\n","    pcc, _ = pearsonr(y_true, y_pred)\n","    return pcc\n","\n","def kendalls_tau_coefficient(y_true, y_pred):\n","    tau, _ = kendalltau(y_true, y_pred)\n","    return tau\n","\n","def evaluate_ranknet_performance(ranknet_model, X_transformed, Y, groups):\n","    evaluation_results = []\n","    unique_groups = np.unique(groups)\n","    for group_id in unique_groups:\n","        idx = groups == group_id\n","        group_features = X_transformed[idx]\n","        group_labels = Y[idx]\n","\n","        mean_features = group_features.mean(axis=0)\n","        normalized_features = group_features - mean_features\n","   \n","        predicted_scores = ranknet_model.predict(normalized_features).flatten()\n","   \n","        ccc_value = concordance_correlation_coefficient(group_labels, predicted_scores)\n","        pcc_value = pearson_correlation_coefficient(group_labels, predicted_scores)\n","        kendall_tau_value = kendalls_tau_coefficient(group_labels, predicted_scores)\n","   \n","        evaluation_results.append({'GroupID': group_id, 'CCC': ccc_value, 'PCC': pcc_value, 'KendallTau': kendall_tau_value})\n","    return pd.DataFrame(evaluation_results)\n","\n","evaluation_results = []  \n","\n","# Initialize the RankNet model\n","ranknet_model = RankNet()\n","ranknet_model.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","excluded_features = [\n","        'VIDEO_40_LLD_AU1', 'VIDEO_40_LLD_AU2', 'VIDEO_40_LLD_AU4', 'VIDEO_40_LLD_AU5', 'VIDEO_40_LLD_AU6', 'VIDEO_40_LLD_AU7', 'VIDEO_40_LLD_AU9', 'VIDEO_40_LLD_AU11', 'VIDEO_40_LLD_AU12', 'VIDEO_40_LLD_AU15',\n","        'VIDEO_40_LLD_AU17', 'VIDEO_40_LLD_AU20', 'VIDEO_40_LLD_AU23', 'VIDEO_40_LLD_AU24', 'VIDEO_40_LLD_AU25', 'VIDEO_40_LLD_Yaw', 'VIDEO_40_LLD_Pitch', 'VIDEO_40_LLD_Roll', 'VIDEO_40_LLD_Opt_mean',\n","        'VIDEO_40_LLD_Opt_std', 'VIDEO_40_LLD_AU1_delta', 'VIDEO_40_LLD_AU2_delta', 'VIDEO_40_LLD_AU4_delta', 'VIDEO_40_LLD_AU5_delta', 'VIDEO_40_LLD_AU6_delta', 'VIDEO_40_LLD_AU7_delta', 'VIDEO_40_LLD_AU9_delta',\n","        'VIDEO_40_LLD_AU11_delta', 'VIDEO_40_LLD_AU12_delta', 'VIDEO_40_LLD_AU15_delta', 'VIDEO_40_LLD_AU17_delta', 'VIDEO_40_LLD_AU20_delta', 'VIDEO_40_LLD_AU23_delta', 'VIDEO_40_LLD_AU24_delta', 'VIDEO_40_LLD_AU25_delta',\n","        'VIDEO_40_LLD_Yaw_delta', 'VIDEO_40_LLD_Pitch_delta', 'VIDEO_40_LLD_Roll_delta', 'VIDEO_40_LLD_Opt_mean_delta', 'VIDEO_40_LLD_Opt_std_delta', 'Face_detection_probability', 'ECG_54_LLD_ECG_HR', 'ECG_54_LLD_ECG_HRV',\n","        'ECG_54_LLD_ECG_zcr', 'ECG_54_LLD_ECG_FFT_1', 'ECG_54_LLD_ECG_FFT_2', 'ECG_54_LLD_ECG_FFT_3', 'ECG_54_LLD_ECG_FFT_4', 'ECG_54_LLD_ECG_FFT_5', 'ECG_54_LLD_ECG_FFT_6', 'ECG_54_LLD_ECG_FFT_7', 'ECG_54_LLD_ECG_FFT_8',\n","        'ECG_54_LLD_ECG_FFT_9', 'ECG_54_LLD_ECG_FFT_10', 'ECG_54_LLD_ECG_FFT_11', 'ECG_54_LLD_ECG_FFT_12', 'ECG_54_LLD_ECG_FFT_entropy', 'ECG_54_LLD_ECG_FFT_mean_frequency', 'ECG_54_LLD_ECG_FFT_slope', 'ECG_54_LLD_ECG_mean',\n","        'ECG_54_LLD_ECG_std', 'ECG_54_LLD_ECG_kurtosis', 'ECG_54_LLD_ECG_skewness', 'ECG_54_LLD_ECG_NSImn', 'ECG_54_LLD_ECG_NLDmn', 'ECG_54_LLD_ECG_VLF', 'ECG_54_LLD_ECG_LF', 'ECG_54_LLD_ECG_HF', 'ECG_54_LLD_ECG_LFHF', 'ECG_54_LLD_ECG_zcr_delta',\n","        'ECG_54_LLD_ECG_FFT_1_delta', 'ECG_54_LLD_ECG_FFT_2_delta', 'ECG_54_LLD_ECG_FFT_3_delta', 'ECG_54_LLD_ECG_FFT_4_delta', 'ECG_54_LLD_ECG_FFT_5_delta', 'ECG_54_LLD_ECG_FFT_6_delta', 'ECG_54_LLD_ECG_FFT_7_delta', 'ECG_54_LLD_ECG_FFT_8_delta',\n","        'ECG_54_LLD_ECG_FFT_9_delta', 'ECG_54_LLD_ECG_FFT_10_delta', 'ECG_54_LLD_ECG_FFT_11_delta', 'ECG_54_LLD_ECG_FFT_12_delta', 'ECG_54_LLD_ECG_FFT_entropy_delta', 'ECG_54_LLD_ECG_FFT_mean_frequency_delta', 'ECG_54_LLD_ECG_FFT_slope_delta', 'ECG_54_LLD_ECG_mean_delta',\n","        'ECG_54_LLD_ECG_std_delta', 'ECG_54_LLD_ECG_kurtosis_delta', 'ECG_54_LLD_ECG_skewness_delta', 'ECG_54_LLD_ECG_NSImn_delta', 'ECG_54_LLD_ECG_NLDmn_delta', 'ECG_54_LLD_ECG_VLF_delta', 'ECG_54_LLD_ECG_LF_delta', 'ECG_54_LLD_ECG_HF_delta', 'ECG_54_LLD_ECG_LFHF_delta',\n","        'EDA_62_LLD_time_code', 'EDA_62_LLD_EDA_slope', 'EDA_62_LLD_EDA_std', 'EDA_62_LLD_SCR_FFT_entropy', 'EDA_62_LLD_SCR_FFT_mean_frequency', 'EDA_62_LLD_EDA_mean', 'EDA_62_LLD_EDA_meanD', 'EDA_62_LLD_EDA_meanDneg', 'EDA_62_LLD_EDA_prop', 'EDA_62_LLD_EDA_Xbound', 'EDA_62_LLD_EDA_kurtosis',\n","        'EDA_62_LLD_EDA_skewness', 'EDA_62_LLD_EDA_NSImn', 'EDA_62_LLD_EDA_NLDmn', 'EDA_62_LLD_SCL_mean', 'EDA_62_LLD_SCL_meanD', 'EDA_62_LLD_SCL_meanDneg', 'EDA_62_LLD_SCL_prop', 'EDA_62_LLD_SCL_Xbound', 'EDA_62_LLD_SCL_kurtosis', 'EDA_62_LLD_SCL_skewness', 'EDA_62_LLD_SCL_NSImn',\n","        'EDA_62_LLD_SCL_NLDmn', 'EDA_62_LLD_SCR_mean', 'EDA_62_LLD_SCR_meanD', 'EDA_62_LLD_SCR_meanDneg', 'EDA_62_LLD_SCR_prop', 'EDA_62_LLD_SCR_Xbound', 'EDA_62_LLD_SCR_kurtosis', 'EDA_62_LLD_SCR_skewness', 'EDA_62_LLD_SCR_NSImn', 'EDA_62_LLD_SCR_NLDmn', 'EDA_62_LLD_EDA_slope_delta',\n","        'EDA_62_LLD_EDA_std_delta', 'EDA_62_LLD_SCR_FFT_entropy_delta', 'EDA_62_LLD_SCR_FFT_mean_frequency_delta', 'EDA_62_LLD_EDA_mean_delta', 'EDA_62_LLD_EDA_meanD_delta', 'EDA_62_LLD_EDA_meanDneg_delta', 'EDA_62_LLD_EDA_prop_delta', 'EDA_62_LLD_EDA_Xbound_delta', 'EDA_62_LLD_EDA_kurtosis_delta',\n","        'EDA_62_LLD_EDA_skewness_delta', 'EDA_62_LLD_EDA_NSImn_delta', 'EDA_62_LLD_EDA_NLDmn_delta', 'EDA_62_LLD_SCL_mean_delta', 'EDA_62_LLD_SCL_meanD_delta', 'EDA_62_LLD_SCL_meanDneg_delta', 'EDA_62_LLD_SCL_prop_delta', 'EDA_62_LLD_SCL_Xbound_delta', 'EDA_62_LLD_SCL_kurtosis_delta', 'EDA_62_LLD_SCL_skewness_delta',\n","        'EDA_62_LLD_SCL_NSImn_delta', 'EDA_62_LLD_SCL_NLDmn_delta', 'EDA_62_LLD_SCR_mean_delta', 'EDA_62_LLD_SCR_meanD_delta', 'EDA_62_LLD_SCR_meanDneg_delta', 'EDA_62_LLD_SCR_prop_delta', 'EDA_62_LLD_SCR_Xbound_delta', 'EDA_62_LLD_SCR_kurtosis_delta', 'EDA_62_LLD_SCR_skewness_delta', 'EDA_62_LLD_SCR_NSImn_delta', 'EDA_62_LLD_SCR_NLDmn_delta'\n","\n","    ]\n","\n","#feature_columns = [col for col in df.columns if col not in excluded_features + ['participant_id','median_arousal', 'median_valence', 'time_window', 'arousal_quartile', 'valence_quartile']]\n","feature_columns = [col for col in df.columns if col not in ['participant_id',\n","                                                     'median_arousal',\n","                                                     'median_valence',\n","                                                     'time_window',\n","                                                     'time in seconds',\n","                                                     'FM1 _x', 'FM2 _x', 'FM3 _x', 'FF1 _x', 'FF2 _x', 'FF3_x',\n","                                                     'FM1 _y', 'FM2 _y', 'FM3 _y', 'FF1 _y', 'FF2 _y', 'FF3_y']]\n","\n","targets = ['valence_quartile']\n","\n","# Initialize GroupKFold\n","group_kfold = GroupKFold(n_splits=5)\n","\n","# Process for valence_quartile\n","for target in targets:\n","    print(f\"\\nProcessing for {target}:\")\n","    X = df[feature_columns].values\n","    Y = df[target].values\n","    groups = df['participant_id'].values\n","    for fold, (train_index, test_index) in enumerate(group_kfold.split(X, Y, groups=groups)):\n","        print(f\"\\nFold {fold+1}/{group_kfold.n_splits} for {target}:\")\n","        X_train, X_test = X[train_index], X[test_index]\n","        Y_train, Y_test = Y[train_index], Y[test_index]\n","        groups_train, groups_test = groups[train_index], groups[test_index]\n","\n","        # Transform both training and testing data along with groups\n","        X_train_transformed, Y_train_transformed, groups_train_transformed = pairwise_transformation(X_train, Y_train, groups_train)\n","        X_test_transformed, Y_test_transformed, groups_test_transformed = pairwise_transformation(X_test, Y_test, groups_test)\n","\n","        ranknet_model.fit(X_train_transformed, Y_train_transformed, epochs=2, verbose=1)\n","\n","        # Insert evaluation after predictions\n","        evaluation_df = evaluate_ranknet_performance(ranknet_model, X_test_transformed, Y_test_transformed, groups_test_transformed)\n","        evaluation_results.append(evaluation_df)\n","        del X_train_transformed, Y_train_transformed\n","        del X_test_transformed, Y_test_transformed     \n","        gc.collect()\n","        \n","        \n","# Combine and save evaluation results\n","combined_results_df = pd.concat(evaluation_results, ignore_index=True)\n","\n","combined_evaluation_results_file = os.path.join( 'ranknet_evaluation_results_valence.csv')\n","combined_results_df.to_csv(combined_evaluation_results_file, index=False)\n","\n","print(\"Individual performance evaluation valence results saved.\")\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNLxh8fMWNilvtS5hLvuXGO","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
